{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop: Softmax.\n",
    "\n",
    "First, recall our `cross-entry loss function`.\n",
    "\n",
    "$\\displaystyle L = -ln(p_c)$\n",
    "\n",
    "where $p_c$ is the predicted probablity for the correct class `c`. And our loss function is look like:\n",
    "\n",
    "$\\displaystyle L = -ln(p_i)$ if i equal to c, else the $L = 0$.\n",
    "\n",
    "Define the $out_s$ as the output of the Softmax function. Then we can calculate the backword of the Softmax layer's phase $\\displaystyle \\frac {\\partial L} {\\partial out_s}$. Which we can know that $out_s$ is a vecotr of 10 probablities. And consider the loss L above.\n",
    "\n",
    "$\\displaystyle \\frac {\\partial L} {\\partial out_s} = -\\frac {1} {p_i}$ if i equal to c, else $\\displaystyle \\frac {\\partial L} {\\partial out_s} = 0$\n",
    "\n",
    "Our initial gradient you saw referenced above:\n",
    "```python\n",
    "gradient = np.zeros(10)\n",
    "gradient[label] = -1 / softmax[label]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate the gradient of $out_s(c)$ with the respect to its total parameters. Let $t_i$ be the total for the class $i$. \n",
    "\n",
    "$\\displaystyle out_s(c) =  \\frac {e ^ {t_c}} {S}$ where $S = \\sum_i e ^ {t_i}$\n",
    "\n",
    "Consider some class $k$ such that $k \\neq c$, the derive of $\\partial out_s(c)$ will be like:\n",
    "\n",
    "$\\displaystyle \\frac {\\partial {out_s(c)}} {\\partial {t_k}} = \\frac {\\partial out_s(c)} {\\partial S} ( \\frac {\\partial S} {\\partial t_k}) = \\frac {-e^{t_c} e^{t_i}} {S^2}$\n",
    "\n",
    "The derivation for $c$ is:\n",
    "\n",
    "$\\displaystyle \\frac {\\partial {out_s(c)}} {\\partial {t_c}} = \\frac {\\partial S e^{t_c} - e^{t_c} \\frac {\\partial S} {\\partial t_c}} {S^2} = \\frac {e^{t_c}(S - e^{t_c})} {S^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, kernels, num_filters):\n",
    "        self.kernels = kernels\n",
    "        self.num_filters = num_filters\n",
    "        # Our filters will be kernels * kernels * num_filters.\n",
    "        self.filters = np.random.randn(num_filters, kernels, kernels) / 9\n",
    "    \n",
    "    def iterate_regions(self, image):\n",
    "        \"\"\"\n",
    "        Generates all possible K x K image regions using vaild padding.\n",
    "        - image is a 2d numpy array\n",
    "        \"\"\"\n",
    "        height, width = image.shape\n",
    "        for h in range(height-self.kernels+1):\n",
    "            for w in range(width-self.kernels+1):\n",
    "                yield image[h:h+self.kernels, w:w+self.kernels], h, w\n",
    "    \n",
    "    def forward(self, layer_input):\n",
    "        \"\"\"\n",
    "        Performs a forward calculate of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions\n",
    "        - imput is a 2D numpy array\n",
    "        \"\"\"\n",
    "        # Only fit for the 2D input.\n",
    "        height, width = layer_input.shape\n",
    "        self.last_input = layer_input\n",
    "        \n",
    "        layer_output = np.zeros((height-self.kernels+1, width-self.kernels+1, self.num_filters))\n",
    "        for image_region, h, w in self.iterate_regions(layer_input):\n",
    "            layer_output[h,w] = np.sum(image_region * self.filters, axis=(1, 2))\n",
    "        return layer_output\n",
    "\n",
    "    def backprop(self, dLdout, learn_rate):\n",
    "        dLdfilters = np.zeros(self.filters.shape)\n",
    "        \n",
    "        for image_region, h, w in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                dLdfilters[f] += dLdout[h, w, f] * image_region\n",
    "        \n",
    "        self.filters -= learn_rate * dLdfilters\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer:\n",
    "    def __init__(self, size=2):\n",
    "        \"\"\"\n",
    "        Max pool layer default size is 2.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "    \n",
    "    def iterate_regions(self, image):\n",
    "        height, width, _ = image.shape\n",
    "        mid_h = height // self.size\n",
    "        mid_w = width // self.size\n",
    "        for h in range(mid_h):\n",
    "            for w in range(mid_w):\n",
    "                image_region = image[h*self.size : self.size*(h+1),\n",
    "                                     w*self.size : self.size*(w+1)]\n",
    "                yield image_region, h, w\n",
    "    \n",
    "    def forward(self, layer_input):\n",
    "        self.last_input = layer_input\n",
    "        height, width, filters = layer_input.shape\n",
    "        layer_output = np.zeros((height//self.size, width//self.size, filters))\n",
    "        for image_region, h, w in self.iterate_regions(layer_input):\n",
    "            layer_output[h, w] = np.max(image_region, axis=(0,1))\n",
    "        return layer_output\n",
    "    \n",
    "    def backprop(self, dLdout):\n",
    "        dLdinput = np.zeros(self.last_input.shape)\n",
    "        \n",
    "        for image_regions, h, w in self.iterate_regions(self.last_input):\n",
    "            h, w, d = image_regions.shape\n",
    "            target = np.max(image_regions, axis=(0,1))\n",
    "            \n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    for c in range(d):\n",
    "                        if image_regions[i, j, c] == target[c]:\n",
    "                            dLdinput[h*2 + i, w*2 + j, c] = dLdout[h, w, c]\n",
    "        return dLdinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, nodes):\n",
    "        \"\"\"\n",
    "        Singletion for our Softmax layer.\n",
    "        \"\"\"\n",
    "        self.biases = np.zeros(nodes)\n",
    "    \n",
    "    def cache_parameter(self, layer_input):\n",
    "        \"\"\"\n",
    "        Calculate the mutpliy of the input and the weights.\n",
    "        \"\"\"\n",
    "        self.last_input_shape = layer_input.shape\n",
    "        if not hasattr(self, \"weights\"):\n",
    "            w, h, d = layer_input.shape\n",
    "            self.weights = np.random.randn(w * h * d, len(self.biases)) / (w * h * d)\n",
    "        self.last_input = layer_input.flatten()\n",
    "        self.last_totals = np.dot(self.last_input, self.weights) + self.biases\n",
    "        return self.last_totals\n",
    "    \n",
    "    def forward(self, layer_input):\n",
    "        \"\"\"\n",
    "        Softmax function return the possibility of the classifications\n",
    "        \"\"\"\n",
    "        totals = self.cache_parameter(layer_input)\n",
    "        exp = np.exp(totals)\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "    \n",
    "    def backprop(self, dLdout, learn_rate):\n",
    "        for i, gradient in enumerate(dLdout):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "            S = np.sum(t_exp)\n",
    "            doutdt = -t_exp[i] * t_exp / S ** 2\n",
    "            doutdt[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "            dtdw = self.last_input #\n",
    "            dtdinputs = self.weights\n",
    "            \n",
    "            dLdt = gradient * doutdt\n",
    "            dLdw = dtdw[np.newaxis].T @ dLdt[np.newaxis]\n",
    "            dLdb = dLdt\n",
    "            dLdinputs = dtdinputs @ dLdt\n",
    "            self.weights -= learn_rate * dLdw\n",
    "            self.biases -= learn_rate * dLdb\n",
    "            \n",
    "            return dLdinputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f87e3f27668>"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_label = mnist.train_labels()\n",
    "conv = ConvLayer(3, 8)\n",
    "conv_output = conv.forward(train_images[0])\n",
    "\n",
    "\n",
    "max_pool = MaxPoolLayer(4)\n",
    "max_pool_output = max_pool.forward(conv_output)\n",
    "plt.imshow(max_pool_output[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft = Softmax(10)\n",
    "out = soft.forward(max_pool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.zeros(len(out))\n",
    "label = train_label[0]\n",
    "ans[label] = - 1 / out[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(OrderedDict):\n",
    "    def __init__(self, *kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.keys = list(self.keys())[::-1]\n",
    "    \n",
    "    def forward(self, image, label):\n",
    "        out = (image / 255) - 0.5\n",
    "        for key in self:\n",
    "            out = self[key].forward(out)\n",
    "        loss = -np.log(out[label]) # Cross-Entropy Loss\n",
    "        acc = 1 if np.argmax(out) == label else 0\n",
    "        \n",
    "        return out, loss, acc\n",
    "    \n",
    "    def train(self, image, label, lr=0.005):\n",
    "        out, loss, acc = self.forward(image, label)\n",
    "        gradient = np.zeros(len(out))\n",
    "        gradient[label] = -1 / out[label]\n",
    "        for k in self.keys:\n",
    "            if k == \"softmax\" or k == \"convlayer\":\n",
    "                gradient = self[k].backprop(gradient, lr)\n",
    "            else:\n",
    "                gradient = self[k].backprop(gradient)\n",
    "        \n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_label = mnist.train_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mode({\n",
    "    \"convlayer\": ConvLayer(3, 8),\n",
    "    \"maxpool\": MaxPoolLayer(),\n",
    "    \"softmax\": Softmax(10)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trainning\n",
      "---- Epoch 1 ----\n",
      "[Step 100] Past 100 steps: Average Loss 2.025 | Accuracy: 49%\n",
      "[Step 200] Past 100 steps: Average Loss 1.963 | Accuracy: 51%\n",
      "[Step 300] Past 100 steps: Average Loss 1.899 | Accuracy: 50%\n",
      "[Step 400] Past 100 steps: Average Loss 1.870 | Accuracy: 63%\n",
      "[Step 500] Past 100 steps: Average Loss 1.781 | Accuracy: 69%\n",
      "[Step 600] Past 100 steps: Average Loss 1.742 | Accuracy: 62%\n",
      "[Step 700] Past 100 steps: Average Loss 1.545 | Accuracy: 72%\n",
      "[Step 800] Past 100 steps: Average Loss 1.620 | Accuracy: 65%\n",
      "[Step 900] Past 100 steps: Average Loss 1.541 | Accuracy: 71%\n",
      "[Step 1000] Past 100 steps: Average Loss 1.464 | Accuracy: 71%\n",
      "---- Epoch 2 ----\n",
      "[Step 100] Past 100 steps: Average Loss 1.480 | Accuracy: 70%\n",
      "[Step 200] Past 100 steps: Average Loss 1.429 | Accuracy: 79%\n",
      "[Step 300] Past 100 steps: Average Loss 1.366 | Accuracy: 79%\n",
      "[Step 400] Past 100 steps: Average Loss 1.288 | Accuracy: 80%\n",
      "[Step 500] Past 100 steps: Average Loss 1.415 | Accuracy: 72%\n",
      "[Step 600] Past 100 steps: Average Loss 1.312 | Accuracy: 79%\n",
      "[Step 700] Past 100 steps: Average Loss 1.244 | Accuracy: 79%\n",
      "[Step 800] Past 100 steps: Average Loss 1.233 | Accuracy: 76%\n",
      "[Step 900] Past 100 steps: Average Loss 1.301 | Accuracy: 70%\n",
      "[Step 1000] Past 100 steps: Average Loss 1.189 | Accuracy: 79%\n",
      "---- Epoch 3 ----\n",
      "[Step 100] Past 100 steps: Average Loss 1.121 | Accuracy: 74%\n",
      "[Step 200] Past 100 steps: Average Loss 1.189 | Accuracy: 75%\n",
      "[Step 300] Past 100 steps: Average Loss 1.207 | Accuracy: 77%\n",
      "[Step 400] Past 100 steps: Average Loss 1.007 | Accuracy: 81%\n",
      "[Step 500] Past 100 steps: Average Loss 1.046 | Accuracy: 85%\n",
      "[Step 600] Past 100 steps: Average Loss 1.036 | Accuracy: 82%\n",
      "[Step 700] Past 100 steps: Average Loss 1.131 | Accuracy: 79%\n",
      "[Step 800] Past 100 steps: Average Loss 0.997 | Accuracy: 81%\n",
      "[Step 900] Past 100 steps: Average Loss 1.136 | Accuracy: 82%\n",
      "[Step 1000] Past 100 steps: Average Loss 1.045 | Accuracy: 73%\n",
      "---- Epoch 4 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.909 | Accuracy: 83%\n",
      "[Step 200] Past 100 steps: Average Loss 0.947 | Accuracy: 79%\n",
      "[Step 300] Past 100 steps: Average Loss 0.927 | Accuracy: 88%\n",
      "[Step 400] Past 100 steps: Average Loss 0.851 | Accuracy: 83%\n",
      "[Step 500] Past 100 steps: Average Loss 0.920 | Accuracy: 81%\n",
      "[Step 600] Past 100 steps: Average Loss 0.909 | Accuracy: 78%\n",
      "[Step 700] Past 100 steps: Average Loss 0.801 | Accuracy: 83%\n",
      "[Step 800] Past 100 steps: Average Loss 0.877 | Accuracy: 77%\n",
      "[Step 900] Past 100 steps: Average Loss 0.808 | Accuracy: 87%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.886 | Accuracy: 81%\n",
      "---- Epoch 5 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.764 | Accuracy: 85%\n",
      "[Step 200] Past 100 steps: Average Loss 0.877 | Accuracy: 78%\n",
      "[Step 300] Past 100 steps: Average Loss 0.826 | Accuracy: 83%\n",
      "[Step 400] Past 100 steps: Average Loss 0.708 | Accuracy: 90%\n",
      "[Step 500] Past 100 steps: Average Loss 0.786 | Accuracy: 84%\n",
      "[Step 600] Past 100 steps: Average Loss 0.811 | Accuracy: 81%\n",
      "[Step 700] Past 100 steps: Average Loss 0.742 | Accuracy: 87%\n",
      "[Step 800] Past 100 steps: Average Loss 0.749 | Accuracy: 84%\n",
      "[Step 900] Past 100 steps: Average Loss 0.778 | Accuracy: 81%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.686 | Accuracy: 86%\n",
      "---- Epoch 6 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.604 | Accuracy: 87%\n",
      "[Step 200] Past 100 steps: Average Loss 0.841 | Accuracy: 78%\n",
      "[Step 300] Past 100 steps: Average Loss 0.736 | Accuracy: 87%\n",
      "[Step 400] Past 100 steps: Average Loss 0.684 | Accuracy: 86%\n",
      "[Step 500] Past 100 steps: Average Loss 0.685 | Accuracy: 88%\n",
      "[Step 600] Past 100 steps: Average Loss 0.758 | Accuracy: 85%\n",
      "[Step 700] Past 100 steps: Average Loss 0.566 | Accuracy: 92%\n",
      "[Step 800] Past 100 steps: Average Loss 0.730 | Accuracy: 81%\n",
      "[Step 900] Past 100 steps: Average Loss 0.720 | Accuracy: 82%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.540 | Accuracy: 94%\n",
      "---- Epoch 7 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.674 | Accuracy: 83%\n",
      "[Step 200] Past 100 steps: Average Loss 0.765 | Accuracy: 80%\n",
      "[Step 300] Past 100 steps: Average Loss 0.594 | Accuracy: 90%\n",
      "[Step 400] Past 100 steps: Average Loss 0.853 | Accuracy: 72%\n",
      "[Step 500] Past 100 steps: Average Loss 0.728 | Accuracy: 80%\n",
      "[Step 600] Past 100 steps: Average Loss 0.636 | Accuracy: 84%\n",
      "[Step 700] Past 100 steps: Average Loss 0.634 | Accuracy: 85%\n",
      "[Step 800] Past 100 steps: Average Loss 0.591 | Accuracy: 87%\n",
      "[Step 900] Past 100 steps: Average Loss 0.566 | Accuracy: 89%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.648 | Accuracy: 84%\n",
      "---- Epoch 8 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.527 | Accuracy: 92%\n",
      "[Step 200] Past 100 steps: Average Loss 0.564 | Accuracy: 90%\n",
      "[Step 300] Past 100 steps: Average Loss 0.420 | Accuracy: 94%\n",
      "[Step 400] Past 100 steps: Average Loss 0.573 | Accuracy: 88%\n",
      "[Step 500] Past 100 steps: Average Loss 0.620 | Accuracy: 84%\n",
      "[Step 600] Past 100 steps: Average Loss 0.518 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.519 | Accuracy: 85%\n",
      "[Step 800] Past 100 steps: Average Loss 0.576 | Accuracy: 86%\n",
      "[Step 900] Past 100 steps: Average Loss 0.652 | Accuracy: 84%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.544 | Accuracy: 88%\n",
      "---- Epoch 9 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.533 | Accuracy: 89%\n",
      "[Step 200] Past 100 steps: Average Loss 0.505 | Accuracy: 87%\n",
      "[Step 300] Past 100 steps: Average Loss 0.445 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.415 | Accuracy: 93%\n",
      "[Step 500] Past 100 steps: Average Loss 0.535 | Accuracy: 87%\n",
      "[Step 600] Past 100 steps: Average Loss 0.500 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.475 | Accuracy: 88%\n",
      "[Step 800] Past 100 steps: Average Loss 0.551 | Accuracy: 86%\n",
      "[Step 900] Past 100 steps: Average Loss 0.454 | Accuracy: 92%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.598 | Accuracy: 82%\n",
      "---- Epoch 10 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.488 | Accuracy: 89%\n",
      "[Step 200] Past 100 steps: Average Loss 0.554 | Accuracy: 85%\n",
      "[Step 300] Past 100 steps: Average Loss 0.456 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.513 | Accuracy: 85%\n",
      "[Step 500] Past 100 steps: Average Loss 0.411 | Accuracy: 92%\n",
      "[Step 600] Past 100 steps: Average Loss 0.635 | Accuracy: 85%\n",
      "[Step 700] Past 100 steps: Average Loss 0.525 | Accuracy: 87%\n",
      "[Step 800] Past 100 steps: Average Loss 0.394 | Accuracy: 92%\n",
      "[Step 900] Past 100 steps: Average Loss 0.576 | Accuracy: 82%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.478 | Accuracy: 90%\n",
      "---- Epoch 11 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.556 | Accuracy: 85%\n",
      "[Step 200] Past 100 steps: Average Loss 0.411 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.533 | Accuracy: 87%\n",
      "[Step 400] Past 100 steps: Average Loss 0.517 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.647 | Accuracy: 78%\n",
      "[Step 600] Past 100 steps: Average Loss 0.579 | Accuracy: 84%\n",
      "[Step 700] Past 100 steps: Average Loss 0.572 | Accuracy: 85%\n",
      "[Step 800] Past 100 steps: Average Loss 0.530 | Accuracy: 82%\n",
      "[Step 900] Past 100 steps: Average Loss 0.555 | Accuracy: 86%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.556 | Accuracy: 84%\n",
      "---- Epoch 12 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.362 | Accuracy: 91%\n",
      "[Step 200] Past 100 steps: Average Loss 0.437 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.516 | Accuracy: 85%\n",
      "[Step 400] Past 100 steps: Average Loss 0.591 | Accuracy: 85%\n",
      "[Step 500] Past 100 steps: Average Loss 0.391 | Accuracy: 89%\n",
      "[Step 600] Past 100 steps: Average Loss 0.530 | Accuracy: 87%\n",
      "[Step 700] Past 100 steps: Average Loss 0.469 | Accuracy: 85%\n",
      "[Step 800] Past 100 steps: Average Loss 0.515 | Accuracy: 86%\n",
      "[Step 900] Past 100 steps: Average Loss 0.461 | Accuracy: 87%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.411 | Accuracy: 90%\n",
      "---- Epoch 13 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.362 | Accuracy: 90%\n",
      "[Step 200] Past 100 steps: Average Loss 0.497 | Accuracy: 86%\n",
      "[Step 300] Past 100 steps: Average Loss 0.420 | Accuracy: 90%\n",
      "[Step 400] Past 100 steps: Average Loss 0.431 | Accuracy: 89%\n",
      "[Step 500] Past 100 steps: Average Loss 0.462 | Accuracy: 87%\n",
      "[Step 600] Past 100 steps: Average Loss 0.398 | Accuracy: 87%\n",
      "[Step 700] Past 100 steps: Average Loss 0.596 | Accuracy: 86%\n",
      "[Step 800] Past 100 steps: Average Loss 0.490 | Accuracy: 83%\n",
      "[Step 900] Past 100 steps: Average Loss 0.430 | Accuracy: 89%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.404 | Accuracy: 87%\n",
      "---- Epoch 14 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.390 | Accuracy: 91%\n",
      "[Step 200] Past 100 steps: Average Loss 0.436 | Accuracy: 89%\n",
      "[Step 300] Past 100 steps: Average Loss 0.618 | Accuracy: 84%\n",
      "[Step 400] Past 100 steps: Average Loss 0.507 | Accuracy: 85%\n",
      "[Step 500] Past 100 steps: Average Loss 0.445 | Accuracy: 90%\n",
      "[Step 600] Past 100 steps: Average Loss 0.445 | Accuracy: 86%\n",
      "[Step 700] Past 100 steps: Average Loss 0.378 | Accuracy: 93%\n",
      "[Step 800] Past 100 steps: Average Loss 0.400 | Accuracy: 90%\n",
      "[Step 900] Past 100 steps: Average Loss 0.438 | Accuracy: 88%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.472 | Accuracy: 86%\n",
      "---- Epoch 15 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.477 | Accuracy: 86%\n",
      "[Step 200] Past 100 steps: Average Loss 0.460 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.492 | Accuracy: 88%\n",
      "[Step 400] Past 100 steps: Average Loss 0.656 | Accuracy: 79%\n",
      "[Step 500] Past 100 steps: Average Loss 0.507 | Accuracy: 85%\n",
      "[Step 600] Past 100 steps: Average Loss 0.444 | Accuracy: 88%\n",
      "[Step 700] Past 100 steps: Average Loss 0.542 | Accuracy: 84%\n",
      "[Step 800] Past 100 steps: Average Loss 0.308 | Accuracy: 94%\n",
      "[Step 900] Past 100 steps: Average Loss 0.349 | Accuracy: 92%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.462 | Accuracy: 88%\n",
      "---- Epoch 16 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.481 | Accuracy: 87%\n",
      "[Step 200] Past 100 steps: Average Loss 0.493 | Accuracy: 87%\n",
      "[Step 300] Past 100 steps: Average Loss 0.412 | Accuracy: 87%\n",
      "[Step 400] Past 100 steps: Average Loss 0.459 | Accuracy: 88%\n",
      "[Step 500] Past 100 steps: Average Loss 0.396 | Accuracy: 91%\n",
      "[Step 600] Past 100 steps: Average Loss 0.330 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.479 | Accuracy: 88%\n",
      "[Step 800] Past 100 steps: Average Loss 0.563 | Accuracy: 83%\n",
      "[Step 900] Past 100 steps: Average Loss 0.422 | Accuracy: 88%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.359 | Accuracy: 93%\n",
      "---- Epoch 17 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.461 | Accuracy: 87%\n",
      "[Step 200] Past 100 steps: Average Loss 0.302 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.317 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.255 | Accuracy: 97%\n",
      "[Step 500] Past 100 steps: Average Loss 0.301 | Accuracy: 91%\n",
      "[Step 600] Past 100 steps: Average Loss 0.346 | Accuracy: 95%\n",
      "[Step 700] Past 100 steps: Average Loss 0.367 | Accuracy: 87%\n",
      "[Step 800] Past 100 steps: Average Loss 0.316 | Accuracy: 93%\n",
      "[Step 900] Past 100 steps: Average Loss 0.543 | Accuracy: 84%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.310 | Accuracy: 90%\n",
      "---- Epoch 18 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.392 | Accuracy: 91%\n",
      "[Step 200] Past 100 steps: Average Loss 0.481 | Accuracy: 89%\n",
      "[Step 300] Past 100 steps: Average Loss 0.422 | Accuracy: 87%\n",
      "[Step 400] Past 100 steps: Average Loss 0.463 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.405 | Accuracy: 89%\n",
      "[Step 600] Past 100 steps: Average Loss 0.356 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.447 | Accuracy: 87%\n",
      "[Step 800] Past 100 steps: Average Loss 0.408 | Accuracy: 91%\n",
      "[Step 900] Past 100 steps: Average Loss 0.435 | Accuracy: 89%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.428 | Accuracy: 88%\n",
      "---- Epoch 19 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.474 | Accuracy: 84%\n",
      "[Step 200] Past 100 steps: Average Loss 0.412 | Accuracy: 88%\n",
      "[Step 300] Past 100 steps: Average Loss 0.424 | Accuracy: 90%\n",
      "[Step 400] Past 100 steps: Average Loss 0.478 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.398 | Accuracy: 91%\n",
      "[Step 600] Past 100 steps: Average Loss 0.346 | Accuracy: 88%\n",
      "[Step 700] Past 100 steps: Average Loss 0.508 | Accuracy: 88%\n",
      "[Step 800] Past 100 steps: Average Loss 0.546 | Accuracy: 84%\n",
      "[Step 900] Past 100 steps: Average Loss 0.394 | Accuracy: 93%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.339 | Accuracy: 91%\n",
      "---- Epoch 20 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.358 | Accuracy: 87%\n",
      "[Step 200] Past 100 steps: Average Loss 0.294 | Accuracy: 94%\n",
      "[Step 300] Past 100 steps: Average Loss 0.339 | Accuracy: 88%\n",
      "[Step 400] Past 100 steps: Average Loss 0.446 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.276 | Accuracy: 96%\n",
      "[Step 600] Past 100 steps: Average Loss 0.328 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.446 | Accuracy: 88%\n",
      "[Step 800] Past 100 steps: Average Loss 0.557 | Accuracy: 85%\n",
      "[Step 900] Past 100 steps: Average Loss 0.269 | Accuracy: 93%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.236 | Accuracy: 93%\n",
      "---- Epoch 21 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.436 | Accuracy: 86%\n",
      "[Step 200] Past 100 steps: Average Loss 0.388 | Accuracy: 87%\n",
      "[Step 300] Past 100 steps: Average Loss 0.361 | Accuracy: 90%\n",
      "[Step 400] Past 100 steps: Average Loss 0.544 | Accuracy: 84%\n",
      "[Step 500] Past 100 steps: Average Loss 0.423 | Accuracy: 89%\n",
      "[Step 600] Past 100 steps: Average Loss 0.366 | Accuracy: 92%\n",
      "[Step 700] Past 100 steps: Average Loss 0.411 | Accuracy: 89%\n",
      "[Step 800] Past 100 steps: Average Loss 0.315 | Accuracy: 90%\n",
      "[Step 900] Past 100 steps: Average Loss 0.412 | Accuracy: 85%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.405 | Accuracy: 88%\n",
      "---- Epoch 22 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.371 | Accuracy: 91%\n",
      "[Step 200] Past 100 steps: Average Loss 0.338 | Accuracy: 93%\n",
      "[Step 300] Past 100 steps: Average Loss 0.340 | Accuracy: 93%\n",
      "[Step 400] Past 100 steps: Average Loss 0.454 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.342 | Accuracy: 92%\n",
      "[Step 600] Past 100 steps: Average Loss 0.401 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.271 | Accuracy: 94%\n",
      "[Step 800] Past 100 steps: Average Loss 0.241 | Accuracy: 95%\n",
      "[Step 900] Past 100 steps: Average Loss 0.385 | Accuracy: 91%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.342 | Accuracy: 87%\n",
      "---- Epoch 23 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.466 | Accuracy: 87%\n",
      "[Step 200] Past 100 steps: Average Loss 0.415 | Accuracy: 87%\n",
      "[Step 300] Past 100 steps: Average Loss 0.282 | Accuracy: 92%\n",
      "[Step 400] Past 100 steps: Average Loss 0.355 | Accuracy: 89%\n",
      "[Step 500] Past 100 steps: Average Loss 0.299 | Accuracy: 94%\n",
      "[Step 600] Past 100 steps: Average Loss 0.399 | Accuracy: 87%\n",
      "[Step 700] Past 100 steps: Average Loss 0.388 | Accuracy: 89%\n",
      "[Step 800] Past 100 steps: Average Loss 0.484 | Accuracy: 85%\n",
      "[Step 900] Past 100 steps: Average Loss 0.410 | Accuracy: 90%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.276 | Accuracy: 94%\n",
      "---- Epoch 24 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.271 | Accuracy: 94%\n",
      "[Step 200] Past 100 steps: Average Loss 0.332 | Accuracy: 92%\n",
      "[Step 300] Past 100 steps: Average Loss 0.259 | Accuracy: 92%\n",
      "[Step 400] Past 100 steps: Average Loss 0.320 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.381 | Accuracy: 89%\n",
      "[Step 600] Past 100 steps: Average Loss 0.295 | Accuracy: 92%\n",
      "[Step 700] Past 100 steps: Average Loss 0.219 | Accuracy: 95%\n",
      "[Step 800] Past 100 steps: Average Loss 0.384 | Accuracy: 88%\n",
      "[Step 900] Past 100 steps: Average Loss 0.445 | Accuracy: 85%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.324 | Accuracy: 90%\n",
      "---- Epoch 25 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.341 | Accuracy: 87%\n",
      "[Step 200] Past 100 steps: Average Loss 0.317 | Accuracy: 90%\n",
      "[Step 300] Past 100 steps: Average Loss 0.292 | Accuracy: 90%\n",
      "[Step 400] Past 100 steps: Average Loss 0.201 | Accuracy: 94%\n",
      "[Step 500] Past 100 steps: Average Loss 0.282 | Accuracy: 94%\n",
      "[Step 600] Past 100 steps: Average Loss 0.285 | Accuracy: 90%\n",
      "[Step 700] Past 100 steps: Average Loss 0.488 | Accuracy: 86%\n",
      "[Step 800] Past 100 steps: Average Loss 0.369 | Accuracy: 91%\n",
      "[Step 900] Past 100 steps: Average Loss 0.368 | Accuracy: 87%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.413 | Accuracy: 90%\n",
      "---- Epoch 26 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.326 | Accuracy: 89%\n",
      "[Step 200] Past 100 steps: Average Loss 0.313 | Accuracy: 92%\n",
      "[Step 300] Past 100 steps: Average Loss 0.267 | Accuracy: 93%\n",
      "[Step 400] Past 100 steps: Average Loss 0.396 | Accuracy: 88%\n",
      "[Step 500] Past 100 steps: Average Loss 0.463 | Accuracy: 90%\n",
      "[Step 600] Past 100 steps: Average Loss 0.257 | Accuracy: 91%\n",
      "[Step 700] Past 100 steps: Average Loss 0.346 | Accuracy: 87%\n",
      "[Step 800] Past 100 steps: Average Loss 0.303 | Accuracy: 93%\n",
      "[Step 900] Past 100 steps: Average Loss 0.294 | Accuracy: 93%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.502 | Accuracy: 83%\n",
      "---- Epoch 27 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.378 | Accuracy: 92%\n",
      "[Step 200] Past 100 steps: Average Loss 0.341 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.319 | Accuracy: 90%\n",
      "[Step 400] Past 100 steps: Average Loss 0.344 | Accuracy: 93%\n",
      "[Step 500] Past 100 steps: Average Loss 0.274 | Accuracy: 91%\n",
      "[Step 600] Past 100 steps: Average Loss 0.274 | Accuracy: 93%\n",
      "[Step 700] Past 100 steps: Average Loss 0.568 | Accuracy: 82%\n",
      "[Step 800] Past 100 steps: Average Loss 0.276 | Accuracy: 94%\n",
      "[Step 900] Past 100 steps: Average Loss 0.463 | Accuracy: 85%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.337 | Accuracy: 89%\n",
      "---- Epoch 28 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.358 | Accuracy: 88%\n",
      "[Step 200] Past 100 steps: Average Loss 0.348 | Accuracy: 92%\n",
      "[Step 300] Past 100 steps: Average Loss 0.342 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.252 | Accuracy: 94%\n",
      "[Step 500] Past 100 steps: Average Loss 0.363 | Accuracy: 89%\n",
      "[Step 600] Past 100 steps: Average Loss 0.427 | Accuracy: 83%\n",
      "[Step 700] Past 100 steps: Average Loss 0.340 | Accuracy: 89%\n",
      "[Step 800] Past 100 steps: Average Loss 0.313 | Accuracy: 91%\n",
      "[Step 900] Past 100 steps: Average Loss 0.337 | Accuracy: 91%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.384 | Accuracy: 91%\n",
      "---- Epoch 29 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.480 | Accuracy: 82%\n",
      "[Step 200] Past 100 steps: Average Loss 0.317 | Accuracy: 90%\n",
      "[Step 300] Past 100 steps: Average Loss 0.352 | Accuracy: 89%\n",
      "[Step 400] Past 100 steps: Average Loss 0.299 | Accuracy: 91%\n",
      "[Step 500] Past 100 steps: Average Loss 0.292 | Accuracy: 89%\n",
      "[Step 600] Past 100 steps: Average Loss 0.324 | Accuracy: 86%\n",
      "[Step 700] Past 100 steps: Average Loss 0.208 | Accuracy: 94%\n",
      "[Step 800] Past 100 steps: Average Loss 0.453 | Accuracy: 88%\n",
      "[Step 900] Past 100 steps: Average Loss 0.385 | Accuracy: 88%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.255 | Accuracy: 89%\n",
      "---- Epoch 30 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.341 | Accuracy: 89%\n",
      "[Step 200] Past 100 steps: Average Loss 0.403 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.393 | Accuracy: 89%\n",
      "[Step 400] Past 100 steps: Average Loss 0.386 | Accuracy: 93%\n",
      "[Step 500] Past 100 steps: Average Loss 0.439 | Accuracy: 87%\n",
      "[Step 600] Past 100 steps: Average Loss 0.242 | Accuracy: 94%\n",
      "[Step 700] Past 100 steps: Average Loss 0.311 | Accuracy: 94%\n",
      "[Step 800] Past 100 steps: Average Loss 0.389 | Accuracy: 84%\n",
      "[Step 900] Past 100 steps: Average Loss 0.342 | Accuracy: 91%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.322 | Accuracy: 90%\n",
      "---- Epoch 31 ----\n",
      "[Step 100] Past 100 steps: Average Loss 0.276 | Accuracy: 90%\n",
      "[Step 200] Past 100 steps: Average Loss 0.365 | Accuracy: 85%\n",
      "[Step 300] Past 100 steps: Average Loss 0.386 | Accuracy: 86%\n",
      "[Step 400] Past 100 steps: Average Loss 0.419 | Accuracy: 89%\n",
      "[Step 500] Past 100 steps: Average Loss 0.595 | Accuracy: 85%\n",
      "[Step 600] Past 100 steps: Average Loss 0.301 | Accuracy: 92%\n",
      "[Step 700] Past 100 steps: Average Loss 0.265 | Accuracy: 94%\n",
      "[Step 800] Past 100 steps: Average Loss 0.356 | Accuracy: 88%\n"
     ]
    }
   ],
   "source": [
    "print('Start trainning')\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"---- Epoch {epoch + 1} ----\")\n",
    "    permutation = np.random.permutation(len(train_images))\n",
    "    train_images = train_images[permutation]\n",
    "    train_label = train_label[permutation]\n",
    "    \n",
    "    loss, num_correct = 0, 0\n",
    "    for i, (image, label) in enumerate(zip(train_images[:1000], train_label[:1000])):\n",
    "        if i % 100 == 99:\n",
    "            print(\n",
    "              '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "              (i + 1, loss / 100, num_correct)\n",
    "            )\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "        l, acc = m.train(image, label)\n",
    "        loss += l\n",
    "        num_correct += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.37104409, 0.04130982, 0.05194859, 0.07570148, 0.06456517,\n",
       "        0.09382119, 0.09299074, 0.10734039, 0.05513165, 0.0461469 ]),\n",
       " 0.9914343775905642,\n",
       " 1)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.forward(train_images[1], train_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f87e3a64828>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = OrderedDict({\"key\": 1, \"add\" :2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo(OrderedDict):\n",
    "    def __init__(self, *kwargs):\n",
    "        super().__init__(*kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Foo({\"kel\": 1, \"key\": 10, \"add\" :2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add', 'key', 'kel']"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
